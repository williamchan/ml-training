{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to linear regression\n",
    "\n",
    "Why study linear regression?\n",
    "* widely used\n",
    "* quick to learn and run\n",
    "* common first step to understanding machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's import numpy and a useful plot function. We will use them to warmup with python and numpy.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warm up with numpy arrays.\n",
    "\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[10, 20], [30, 40]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experiment with the numpy array functions. Make sure you understand the difference between * and dot.\n",
    "\n",
    "# print 'dimensions of a =', np.shape(a)\n",
    "# print a + b\n",
    "# print a * b\n",
    "# print a.dot(b)\n",
    "# print a.dot(b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's generate some sample data\n",
    "\n",
    "data_x = np.linspace(1.0, 10.0, 100)[:, np.newaxis]\n",
    "data_y = abs(np.sin(data_x) + 0.1 * np.power(data_x, 2) + 0.5 * np.random.randn(100, 1))\n",
    "data_x /= np.max(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect data_x by printing its first 10 elements and its length\n",
    "print \"first 10 elements of data_x are \", data_x[:10]\n",
    "print \"length of data_x is \", len(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot data_x and data_y\n",
    "plt.scatter(data_x, data_y, c='g', label='Lighthouse BBQ')\n",
    "plt.grid()\n",
    "plt.xlabel('City population in millions')\n",
    "plt.ylabel('$ in thousands')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-variable linear regression\n",
    "\n",
    "Pretend we run a food truck. Every night, we drive to a different city and make money serving Lighthouse BBQ. The Y-axis shows how much money we make when we work on a typical night. The X-axis shows how many people live in that city in units of 10,000.\n",
    "\n",
    "We think our profits are tied to the population sizes of the cities we serve. How can we try to predict how much money we will make based on which cities we work in? By drawing a line through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Actually, we will draw a line through most of the data and then test \n",
    "# our regression line against the remaining part of the data.\n",
    "# We call these two partitions the training data and the test data.\n",
    "# An 80/20 split between training data and test data is common.\n",
    "\n",
    "data_x = np.hstack((np.ones_like(data_x), data_x)) # Add a row of 1s to multiply with the constant parameter.\n",
    "\n",
    "randomized_indices = np.random.permutation(len(data_x))\n",
    "\n",
    "test_x = data_x[randomized_indices[:20]]\n",
    "test_y = data_y[randomized_indices[:20]]\n",
    "train_x = data_x[randomized_indices[20:]]\n",
    "train_y = data_y[randomized_indices[20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feel free to check that we have partitioned the data correctly.\n",
    "print \"size of train_x =\", len(train_x)\n",
    "print \"size of train_y =\", len(train_y)\n",
    "print \"size of test_x =\", len(test_x)\n",
    "print \"size of test_y =\", len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equation of a line\n",
    "\n",
    "In algebra, it looks like\n",
    ">  y = a*x + b\n",
    "\n",
    "In linear algebra, it looks like\n",
    ">  y = w*x\n",
    "\n",
    "where w and x are both matrixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The goal is to minimize the MSE loss function. Calculus teaches us when a function has a\n",
    "# nice bowl shape (\"convex\"), we can head toward the minimum by moving in the opposite \n",
    "# direction of the derivative. Because there are typically multiple dimensions, we use the \n",
    "# word \"gradient\" instead of \"derivative.\" Gradient means the derivative of a function \n",
    "# with respect to multiple dimensions, and iteratively moving towards the bottom of a bowl\n",
    "# still works.\n",
    "\n",
    "# This is the gradient formula for linear regression. \n",
    "# If you want to see the calculus proof, see page 9 of \n",
    "# http://cs229.stanford.edu/notes/cs229-notes1.pdf.\n",
    "def get_gradient(w, x, y):\n",
    "    y_estimate = x.dot(w).flatten()\n",
    "    error = (y.flatten() - y_estimate)\n",
    "    mse = (1.0/len(x)) * np.sum(error ** 2)\n",
    "    gradient = -(1.0/len(x)) * error.dot(x)\n",
    "    return gradient, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize w to random values. Gradient descent will iteratively move w closer to the right answer!\n",
    "def train(train_x, train_y, learning_rate=0.5, num_iterations=300):\n",
    "  num_dimensions = np.shape(test_x)[1]\n",
    "  w = np.random.randn(num_dimensions)\n",
    "\n",
    "  # Perform gradient descent\n",
    "  iteration = 1\n",
    "  while iteration < num_iterations:\n",
    "    gradient, error = get_gradient(w, train_x, train_y)\n",
    "    new_w = w - learning_rate * gradient\n",
    "    \n",
    "    if iteration % 30 == 0:\n",
    "      print \"Iteration: %d - Error: %.4f\" %(iteration, error)\n",
    "    \n",
    "    iteration += 1\n",
    "    w = new_w\n",
    "\n",
    "  print \"w =\", w\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = train(train_x, train_y, 0.5, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the model trained from the previous step, print the mean squared error of the regression\n",
    "# against the test set. This will give a numeric indicator of how good the model is at predicting\n",
    "# outcomes, ie how much money Lighthouse BBQ will make in a given city.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A visual plot of the regression against the training and test data.\n",
    "\n",
    "plt.plot(data_x[:,1], data_x.dot(w), c='g', label='Model')\n",
    "plt.scatter(train_x[:,1], train_y, c='b', label='Train Set')\n",
    "plt.scatter(test_x[:,1], test_y, c='r', label='Test Set')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('City population in millions')\n",
    "plt.ylabel('$ in thousands')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's also visualize the cost function\n",
    "\n",
    "w1 = np.linspace(-w[1]*3, w[1]*3, 300)\n",
    "w0 = np.linspace(-w[0]*3, w[0]*3, 300)\n",
    "J_vals = np.zeros(shape=(w1.size, w0.size))\n",
    "\n",
    "for t1, e1 in enumerate(w1):\n",
    "    for t2, e2 in enumerate(w0):\n",
    "        wT = [0, 0]\n",
    "        wT[1] = e1\n",
    "        wT[0] = e2\n",
    "        J_vals[t1, t2] = get_gradient(wT, train_x, train_y)[1]\n",
    "\n",
    "plt.scatter(w[0], w[1], marker='*', color='r', s=40, label='Solution Found')\n",
    "CS = plt.contour(w0, w1, J_vals, np.logspace(-10,10,50), label='Cost Function')\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "plt.title(\"Contour Plot of Cost Function\")\n",
    "plt.xlabel(\"w0\")\n",
    "plt.ylabel(\"w1\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your turn to write some code. Implement a method that returns a prediction when given \n",
    "# trained weights and inputs.\n",
    "def predict(w, x):\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your predict method. How much money will be made in a city with population of 0.5 million?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariable linear regression\n",
    "\n",
    "Now let's move to multiple dimensions. The following generates sample training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_x = np.linspace(1.0, 10.0, 100)[:, np.newaxis]\n",
    "data_y = abs(np.sin(data_x) + 0.1 * np.power(data_x, 2) + 0.5 * np.random.randn(100, 1))\n",
    "data_x /= np.max(data_x)\n",
    "data_x = np.hstack((np.ones_like(data_x), data_x, np.random.randn(100, 1)))\n",
    "\n",
    "randomized_indices = np.random.permutation(len(data_x))\n",
    "\n",
    "test_x = data_x[randomized_indices[:20]]\n",
    "test_y = data_y[randomized_indices[:20]]\n",
    "train_x = data_x[randomized_indices[20:]]\n",
    "train_y = data_y[randomized_indices[20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The same train method should work even with more dimensions.\n",
    "\n",
    "w = train(train_x, train_y, 0.5, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Again, test your predict method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "* Did you see what you expected to see?\n",
    "* How is linear regression used in the world?\n",
    "* In real life, where do we get the dimensions aka features?\n",
    "* Feature selection.\n",
    "* Dimension normalization.\n",
    "* Bias and variance.\n",
    "* Underfitting and overfitting.\n",
    "* How to tune a model: http://cs229.stanford.edu/materials/ML-advice.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
